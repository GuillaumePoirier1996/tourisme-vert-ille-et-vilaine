{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¢üì¢üì¢ Projet NLP : La parole aux citoyens (Make.org - Le tourisme vert en Ille-et-Vilaine)üì¢üì¢üì¢\n",
    "# Preprocessing NLP\n",
    "## Biblioth√®ques n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from nltk.data import find\n",
    "import spacy\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T√©l√©chargements des utilitaires si n√©cessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Dossier local de ressources NLTK (dans ton projet ou venv)\n",
    "NLTK_LOCAL_PATH = '../src/nltk_data'\n",
    "\n",
    "# üîÑ Ajouter ce dossier au chemin de recherche\n",
    "nltk.data.path.append(NLTK_LOCAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Fonction utilitaire pour t√©l√©charger uniquement si n√©cessaire\n",
    "def safe_nltk_download(resource_name, download_dir=NLTK_LOCAL_PATH):\n",
    "    try:\n",
    "        find(resource_name)\n",
    "    except LookupError:\n",
    "        nltk.download(resource_name, download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ../src/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to ../src/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     ../src/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     ../src/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ../src/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# üìö T√©l√©chargements n√©cessaires\n",
    "safe_nltk_download('punkt')  # Tokenizer\n",
    "safe_nltk_download('punkt_tab')  # Tokenizer\n",
    "safe_nltk_download('averaged_perceptron_tagger')  # POS tagger anglais\n",
    "safe_nltk_download('universal_tagset')  # Universal tag mapping\n",
    "safe_nltk_download('stopwords')  # Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))  # Charger les stop words pour le fran√ßais\n",
    "nlp = spacy.load('fr_core_news_sm') # Chargement du mod√®le de langue fran√ßaise de SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>agree_count</th>\n",
       "      <th>disagree_count</th>\n",
       "      <th>neutral_count</th>\n",
       "      <th>agree_score</th>\n",
       "      <th>disagree_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>doNotCare</th>\n",
       "      <th>doNotUnderstand</th>\n",
       "      <th>doable</th>\n",
       "      <th>impossible</th>\n",
       "      <th>likeIt</th>\n",
       "      <th>noOpinion</th>\n",
       "      <th>noWay</th>\n",
       "      <th>platitudeAgree</th>\n",
       "      <th>platitudeDisagree</th>\n",
       "      <th>nb_interrogations</th>\n",
       "      <th>nb_exclamation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Il faut une surveillance des lieux touristique...</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Il faut donner des sacs poubelles et des cendr...</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Il faut r√©compenser et mettre en avant les act...</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  agree_count  \\\n",
       "0  Il faut une surveillance des lieux touristique...           27   \n",
       "1  Il faut donner des sacs poubelles et des cendr...           56   \n",
       "2  Il faut r√©compenser et mettre en avant les act...           35   \n",
       "\n",
       "   disagree_count  neutral_count  agree_score  disagree_score  neutral_score  \\\n",
       "0               7              7         0.65            0.18           0.17   \n",
       "1              18             15         0.62            0.19           0.19   \n",
       "2               7             14         0.62            0.13           0.25   \n",
       "\n",
       "   doNotCare  doNotUnderstand  doable  impossible  likeIt  noOpinion  noWay  \\\n",
       "0          1                0       8           2       2          0      3   \n",
       "1          0                0      21           6       6          5      4   \n",
       "2          0                5      12           2       2          2      3   \n",
       "\n",
       "   platitudeAgree  platitudeDisagree  nb_interrogations  nb_exclamation  \n",
       "0               3                  4                  0               0  \n",
       "1               5                  7                  0               0  \n",
       "2               1                  5                  0               0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../data/processed/req_tourisme_responsable.csv\",\n",
    "    delimiter=\";\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Tokenisation avec NLTK (obtenir un decoupage pour chaque mot) pour garder les mots en majuscules\n",
    "- Tokenisation avec GENSIM en utilisant simple preprocess pour normaliser (enlever les accents, mettre en minuscules, ...)\n",
    "### Avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser chaque document dans la colonne 'texte'\n",
    "df['tokens'] = df['content'].apply(lambda x: word_tokenize(x, language='french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Il, faut, une, surveillance, des, lieux, tour...\n",
       "1    [Il, faut, donner, des, sacs, poubelles, et, d...\n",
       "2    [Il, faut, r√©compenser, et, mettre, en, avant,...\n",
       "3    [Il, faut, promouvoir, toutes, les, destinatio...\n",
       "4    [Il, faut, enrayer, la, sp√©culation, immobili√®...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la colonne 'case' en v√©rifiant si chaque token est en majuscules ou en minuscules\n",
    "df['case'] = df['tokens'].apply(lambda tokens: ['Maj' if token.isupper() else 'Min' for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser chaque document dans la colonne 'texte'\n",
    "df['tokens_normalized'] = df['content'].apply(lambda x: simple_preprocess(x, deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>tokens_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, une, surveillance, des, lieux, tour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, donner, des, sacs, poubelles, et, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, recompenser, et, mettre, en, avant,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, promouvoir, toutes, les, destinatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, enrayer, la, speculation, immobilie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                case  \\\n",
       "0  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "1  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "2  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "3  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "4  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "\n",
       "                                   tokens_normalized  \n",
       "0  [il, faut, une, surveillance, des, lieux, tour...  \n",
       "1  [il, faut, donner, des, sacs, poubelles, et, d...  \n",
       "2  [il, faut, recompenser, et, mettre, en, avant,...  \n",
       "3  [il, faut, promouvoir, toutes, les, destinatio...  \n",
       "4  [il, faut, enrayer, la, speculation, immobilie...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['case','tokens_normalized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les stop words des tokens normalis√©s et stocker les mots retir√©s dans une nouvelle colonne\n",
    "df['stop_words'] = df['tokens_normalized'].apply(lambda tokens: [token for token in tokens if token in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_normalized</th>\n",
       "      <th>stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[il, faut, une, surveillance, des, lieux, tour...</td>\n",
       "      <td>[il, une, des, avec, la, pour, ou]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[il, faut, donner, des, sacs, poubelles, et, d...</td>\n",
       "      <td>[il, des, et, des, de, aux, des, et, en, de]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[il, faut, recompenser, et, mettre, en, avant,...</td>\n",
       "      <td>[il, et, en, les, des, du, et, des, pour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[il, faut, promouvoir, toutes, les, destinatio...</td>\n",
       "      <td>[il, les, et, le]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[il, faut, enrayer, la, speculation, immobilie...</td>\n",
       "      <td>[il, la, en, de, pour, un]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tokens_normalized  \\\n",
       "0  [il, faut, une, surveillance, des, lieux, tour...   \n",
       "1  [il, faut, donner, des, sacs, poubelles, et, d...   \n",
       "2  [il, faut, recompenser, et, mettre, en, avant,...   \n",
       "3  [il, faut, promouvoir, toutes, les, destinatio...   \n",
       "4  [il, faut, enrayer, la, speculation, immobilie...   \n",
       "\n",
       "                                     stop_words  \n",
       "0            [il, une, des, avec, la, pour, ou]  \n",
       "1  [il, des, et, des, de, aux, des, et, en, de]  \n",
       "2     [il, et, en, les, des, du, et, des, pour]  \n",
       "3                             [il, les, et, le]  \n",
       "4                    [il, la, en, de, pour, un]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['tokens_normalized','stop_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les deux premiers termes de chaque liste dans la colonne 'tokens_normalized'\n",
    "df['tokens_normalized'] = df['tokens_normalized'].apply(lambda tokens: tokens[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [une, surveillance, des, lieux, touristiques, ...\n",
       "1    [donner, des, sacs, poubelles, et, des, cendri...\n",
       "2    [recompenser, et, mettre, en, avant, les, acte...\n",
       "3    [promouvoir, toutes, les, destinations, et, de...\n",
       "4    [enrayer, la, speculation, immobiliere, en, te...\n",
       "Name: tokens_normalized, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_normalized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les stop words des tokens normalis√©s\n",
    "df['tokens_final'] = df['tokens_normalized'].apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [surveillance, lieux, touristiques, amende, cl...\n",
       "1    [donner, sacs, poubelles, cendriers, poche, va...\n",
       "2    [recompenser, mettre, avant, actes, projets, e...\n",
       "3    [promouvoir, toutes, destinations, developper,...\n",
       "4    [enrayer, speculation, immobiliere, territoire...\n",
       "Name: tokens_final, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_final'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation avec spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lemmatiser les tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    # Joindre les tokens en une cha√Æne de caract√®res\n",
    "    text = ' '.join(tokens)\n",
    "    # L'objet nlp traite le texte\n",
    "    doc = nlp(text)\n",
    "    # Lemmatiser chaque token\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la lemmatisation √† chaque ligne de la colonne 'tokens_normalized'\n",
    "df['tokens_lemmatized'] = df['tokens_final'].apply(lambda x : lemmatize_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [surveillance, lieux, touristique, amende, cle...\n",
       "1    [donner, sac, poubelle, cendrier, poch, vacanc...\n",
       "2    [recompenser, mettre, avant, acte, projet, eco...\n",
       "3    [promouvoir, tout, destination, developper, to...\n",
       "4    [enrayer, speculation, immobilier, territoire,...\n",
       "Name: tokens_lemmatized, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_lemmatized'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suite √† plusieurs visualisations de nuage de mots, enrichissement des stop_words\n",
    "- les d√©rive de tourisme\n",
    "- tout\n",
    "- faire\n",
    "- plus\n",
    "\n",
    "Ces mots n'apportent pas grand-chose sur le sujet. Donc on les enl√®ve des tokens lemmatis√©s. Bien s√ªr, ce processus pourrait continuer de mani√®re it√©rative, mais il faut savoir s'arr√™ter.\n",
    "\n",
    "üîç Pourquoi j'ai choisi d'enlever les stop words avant vectorisation ?\n",
    "Les stop words (comme le, la, de, et, que...) :\n",
    "- n‚Äôapportent pas d'information s√©mantique forte ;\n",
    "- sont trop fr√©quents ‚Üí ils polluent l‚Äôapprentissage de Word2Vec ;\n",
    "- g√©n√®rent des vecteurs tr√®s proches entre eux, peu utiles pour des t√¢ches de similarit√© ou de clustering.\n",
    "\n",
    "Word2Vec apprend √† pr√©dire un mot √† partir du contexte (ou l‚Äôinverse). Les stop words, tr√®s fr√©quents, apparaissent dans presque tous les contextes ‚Üí ils biaisent l‚Äôapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add('tourisme')\n",
    "stop_words.add('touristique')\n",
    "stop_words.add('touriste')\n",
    "stop_words.add('tourism')\n",
    "stop_words.add(\"tout\")\n",
    "stop_words.add(\"plus\")\n",
    "stop_words.add(\"faire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les stop words des tokens normalis√©s\n",
    "df['tokens_final'] = df['tokens_lemmatized'].apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde du dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repertoire de sauvegarde\n",
    "savepath = \"../data/processed\"\n",
    "\n",
    "# Test de l'existence du r√©pertoire\n",
    "if not os.path.exists(savepath):\n",
    "    # Cr√©ation du r√©pertoire s'il n'existe pas\n",
    "    os.makedirs(savepath)\n",
    "\n",
    "# Sauvegarde des jeux de donn√©es\n",
    "df.to_csv(\n",
    "    path_or_buf=os.path.join(savepath, \"req_tourisme_responsable_tokenised.csv\"),\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\",\n",
    "    index_label=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tourisme-vert-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
