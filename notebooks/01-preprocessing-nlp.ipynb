{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📢📢📢 Projet NLP : La parole aux citoyens (Make.org - Le tourisme vert en Ille-et-Vilaine)📢📢📢\n",
    "# Preprocessing NLP\n",
    "## Bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from nltk.data import find\n",
    "import spacy\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargements des utilitaires si nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Dossier local de ressources NLTK (dans ton projet ou venv)\n",
    "NLTK_LOCAL_PATH = '../src/nltk_data'\n",
    "\n",
    "# 🔄 Ajouter ce dossier au chemin de recherche\n",
    "nltk.data.path.append(NLTK_LOCAL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📥 Fonction utilitaire pour télécharger uniquement si nécessaire\n",
    "def safe_nltk_download(resource_name, download_dir=NLTK_LOCAL_PATH):\n",
    "    try:\n",
    "        find(resource_name)\n",
    "    except LookupError:\n",
    "        nltk.download(resource_name, download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ../src/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to ../src/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     ../src/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     ../src/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ../src/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 📚 Téléchargements nécessaires\n",
    "safe_nltk_download('punkt')  # Tokenizer\n",
    "safe_nltk_download('punkt_tab')  # Tokenizer\n",
    "safe_nltk_download('averaged_perceptron_tagger')  # POS tagger anglais\n",
    "safe_nltk_download('universal_tagset')  # Universal tag mapping\n",
    "safe_nltk_download('stopwords')  # Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))  # Charger les stop words pour le français\n",
    "nlp = spacy.load('fr_core_news_sm') # Chargement du modèle de langue française de SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>agree_count</th>\n",
       "      <th>disagree_count</th>\n",
       "      <th>neutral_count</th>\n",
       "      <th>agree_score</th>\n",
       "      <th>disagree_score</th>\n",
       "      <th>neutral_score</th>\n",
       "      <th>doNotCare</th>\n",
       "      <th>doNotUnderstand</th>\n",
       "      <th>doable</th>\n",
       "      <th>impossible</th>\n",
       "      <th>likeIt</th>\n",
       "      <th>noOpinion</th>\n",
       "      <th>noWay</th>\n",
       "      <th>platitudeAgree</th>\n",
       "      <th>platitudeDisagree</th>\n",
       "      <th>nb_interrogations</th>\n",
       "      <th>nb_exclamation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Il faut une surveillance des lieux touristique...</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Il faut donner des sacs poubelles et des cendr...</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Il faut récompenser et mettre en avant les act...</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  agree_count  \\\n",
       "0  Il faut une surveillance des lieux touristique...           27   \n",
       "1  Il faut donner des sacs poubelles et des cendr...           56   \n",
       "2  Il faut récompenser et mettre en avant les act...           35   \n",
       "\n",
       "   disagree_count  neutral_count  agree_score  disagree_score  neutral_score  \\\n",
       "0               7              7         0.65            0.18           0.17   \n",
       "1              18             15         0.62            0.19           0.19   \n",
       "2               7             14         0.62            0.13           0.25   \n",
       "\n",
       "   doNotCare  doNotUnderstand  doable  impossible  likeIt  noOpinion  noWay  \\\n",
       "0          1                0       8           2       2          0      3   \n",
       "1          0                0      21           6       6          5      4   \n",
       "2          0                5      12           2       2          2      3   \n",
       "\n",
       "   platitudeAgree  platitudeDisagree  nb_interrogations  nb_exclamation  \n",
       "0               3                  4                  0               0  \n",
       "1               5                  7                  0               0  \n",
       "2               1                  5                  0               0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../data/processed/req_tourisme_responsable.csv\",\n",
    "    delimiter=\";\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Tokenisation avec NLTK (obtenir un decoupage pour chaque mot) pour garder les mots en majuscules\n",
    "- Tokenisation avec GENSIM en utilisant simple preprocess pour normaliser (enlever les accents, mettre en minuscules, ...)\n",
    "### Avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser chaque document dans la colonne 'texte'\n",
    "df['tokens'] = df['content'].apply(lambda x: word_tokenize(x, language='french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Il, faut, une, surveillance, des, lieux, tour...\n",
       "1    [Il, faut, donner, des, sacs, poubelles, et, d...\n",
       "2    [Il, faut, récompenser, et, mettre, en, avant,...\n",
       "3    [Il, faut, promouvoir, toutes, les, destinatio...\n",
       "4    [Il, faut, enrayer, la, spéculation, immobiliè...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la colonne 'case' en vérifiant si chaque token est en majuscules ou en minuscules\n",
    "df['case'] = df['tokens'].apply(lambda tokens: ['Maj' if token.isupper() else 'Min' for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser chaque document dans la colonne 'texte'\n",
    "df['tokens_normalized'] = df['content'].apply(lambda x: simple_preprocess(x, deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>tokens_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, une, surveillance, des, lieux, tour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, donner, des, sacs, poubelles, et, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, recompenser, et, mettre, en, avant,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, promouvoir, toutes, les, destinatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
       "      <td>[il, faut, enrayer, la, speculation, immobilie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                case  \\\n",
       "0  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "1  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "2  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "3  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "4  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
       "\n",
       "                                   tokens_normalized  \n",
       "0  [il, faut, une, surveillance, des, lieux, tour...  \n",
       "1  [il, faut, donner, des, sacs, poubelles, et, d...  \n",
       "2  [il, faut, recompenser, et, mettre, en, avant,...  \n",
       "3  [il, faut, promouvoir, toutes, les, destinatio...  \n",
       "4  [il, faut, enrayer, la, speculation, immobilie...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['case','tokens_normalized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les stop words des tokens normalisés et stocker les mots retirés dans une nouvelle colonne\n",
    "df['stop_words'] = df['tokens_normalized'].apply(lambda tokens: [token for token in tokens if token in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_normalized</th>\n",
       "      <th>stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[il, faut, une, surveillance, des, lieux, tour...</td>\n",
       "      <td>[il, une, des, avec, la, pour, ou]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[il, faut, donner, des, sacs, poubelles, et, d...</td>\n",
       "      <td>[il, des, et, des, de, aux, des, et, en, de]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[il, faut, recompenser, et, mettre, en, avant,...</td>\n",
       "      <td>[il, et, en, les, des, du, et, des, pour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[il, faut, promouvoir, toutes, les, destinatio...</td>\n",
       "      <td>[il, les, et, le]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[il, faut, enrayer, la, speculation, immobilie...</td>\n",
       "      <td>[il, la, en, de, pour, un]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tokens_normalized  \\\n",
       "0  [il, faut, une, surveillance, des, lieux, tour...   \n",
       "1  [il, faut, donner, des, sacs, poubelles, et, d...   \n",
       "2  [il, faut, recompenser, et, mettre, en, avant,...   \n",
       "3  [il, faut, promouvoir, toutes, les, destinatio...   \n",
       "4  [il, faut, enrayer, la, speculation, immobilie...   \n",
       "\n",
       "                                     stop_words  \n",
       "0            [il, une, des, avec, la, pour, ou]  \n",
       "1  [il, des, et, des, de, aux, des, et, en, de]  \n",
       "2     [il, et, en, les, des, du, et, des, pour]  \n",
       "3                             [il, les, et, le]  \n",
       "4                    [il, la, en, de, pour, un]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['tokens_normalized','stop_words']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les deux premiers termes de chaque liste dans la colonne 'tokens_normalized'\n",
    "df['tokens_normalized'] = df['tokens_normalized'].apply(lambda tokens: tokens[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [une, surveillance, des, lieux, touristiques, ...\n",
       "1    [donner, des, sacs, poubelles, et, des, cendri...\n",
       "2    [recompenser, et, mettre, en, avant, les, acte...\n",
       "3    [promouvoir, toutes, les, destinations, et, de...\n",
       "4    [enrayer, la, speculation, immobiliere, en, te...\n",
       "Name: tokens_normalized, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_normalized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les stop words des tokens normalisés\n",
    "df['tokens_final'] = df['tokens_normalized'].apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [surveillance, lieux, touristiques, amende, cl...\n",
       "1    [donner, sacs, poubelles, cendriers, poche, va...\n",
       "2    [recompenser, mettre, avant, actes, projets, e...\n",
       "3    [promouvoir, toutes, destinations, developper,...\n",
       "4    [enrayer, speculation, immobiliere, territoire...\n",
       "Name: tokens_final, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_final'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation avec spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lemmatiser les tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    # Joindre les tokens en une chaîne de caractères\n",
    "    text = ' '.join(tokens)\n",
    "    # L'objet nlp traite le texte\n",
    "    doc = nlp(text)\n",
    "    # Lemmatiser chaque token\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la lemmatisation à chaque ligne de la colonne 'tokens_normalized'\n",
    "df['tokens_lemmatized'] = df['tokens_final'].apply(lambda x : lemmatize_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [surveillance, lieux, touristique, amende, cle...\n",
       "1    [donner, sac, poubelle, cendrier, poch, vacanc...\n",
       "2    [recompenser, mettre, avant, acte, projet, eco...\n",
       "3    [promouvoir, tout, destination, developper, to...\n",
       "4    [enrayer, speculation, immobilier, territoire,...\n",
       "Name: tokens_lemmatized, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_lemmatized'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suite à plusieurs visualisations de nuage de mots, enrichissement des stop_words\n",
    "- les dérive de tourisme\n",
    "- tout\n",
    "- faire\n",
    "- plus\n",
    "\n",
    "Ces mots n'apportent pas grand-chose sur le sujet. Donc on les enlève des tokens lemmatisés. Bien sûr, ce processus pourrait continuer de manière itérative, mais il faut savoir s'arrêter.\n",
    "\n",
    "🔍 Pourquoi j'ai choisi d'enlever les stop words avant vectorisation ?\n",
    "Les stop words (comme le, la, de, et, que...) :\n",
    "- n’apportent pas d'information sémantique forte ;\n",
    "- sont trop fréquents → ils polluent l’apprentissage de Word2Vec ;\n",
    "- génèrent des vecteurs très proches entre eux, peu utiles pour des tâches de similarité ou de clustering.\n",
    "\n",
    "Word2Vec apprend à prédire un mot à partir du contexte (ou l’inverse). Les stop words, très fréquents, apparaissent dans presque tous les contextes → ils biaisent l’apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add('tourisme')\n",
    "stop_words.add('touristique')\n",
    "stop_words.add('touriste')\n",
    "stop_words.add('tourism')\n",
    "stop_words.add(\"tout\")\n",
    "stop_words.add(\"plus\")\n",
    "stop_words.add(\"faire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les stop words des tokens normalisés\n",
    "df['tokens_final'] = df['tokens_lemmatized'].apply(lambda tokens: [token for token in tokens if token not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde du dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repertoire de sauvegarde\n",
    "savepath = \"../data/processed\"\n",
    "\n",
    "# Test de l'existence du répertoire\n",
    "if not os.path.exists(savepath):\n",
    "    # Création du répertoire s'il n'existe pas\n",
    "    os.makedirs(savepath)\n",
    "\n",
    "# Sauvegarde des jeux de données\n",
    "df.to_csv(\n",
    "    path_or_buf=os.path.join(savepath, \"req_tourisme_responsable_tokenised.csv\"),\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\",\n",
    "    index_label=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tourisme-vert-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
