{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udce2\ud83d\udce2\ud83d\udce2 Projet NLP : La parole aux citoyens (Make.org - Le tourisme vert en Ille-et-Vilaine)\ud83d\udce2\ud83d\udce2\ud83d\udce2\n",
        "# Preprocessing NLP\n",
        "## Biblioth\u00e8ques n\u00e9cessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.data import find"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## T\u00e9l\u00e9chargements des utilitaires si n\u00e9cessaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd27 Dossier local de ressources NLTK (dans ton projet ou venv)\n",
        "NLTK_LOCAL_PATH = \"../src/nltk_data\"\n",
        "\n",
        "# \ud83d\udd04 Ajouter ce dossier au chemin de recherche\n",
        "nltk.data.path.append(NLTK_LOCAL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udce5 Fonction utilitaire pour t\u00e9l\u00e9charger uniquement si n\u00e9cessaire\n",
        "def safe_nltk_download(resource_name, download_dir=NLTK_LOCAL_PATH):\n",
        "    try:\n",
        "        find(resource_name)\n",
        "    except LookupError:\n",
        "        nltk.download(resource_name, download_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to ../src/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to ../src/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     ../src/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to\n",
            "[nltk_data]     ../src/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to ../src/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# \ud83d\udcda T\u00e9l\u00e9chargements n\u00e9cessaires\n",
        "safe_nltk_download(\"punkt\")  # Tokenizer\n",
        "safe_nltk_download(\"punkt_tab\")  # Tokenizer\n",
        "safe_nltk_download(\"averaged_perceptron_tagger\")  # POS tagger anglais\n",
        "safe_nltk_download(\"universal_tagset\")  # Universal tag mapping\n",
        "safe_nltk_download(\"stopwords\")  # Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words(\"french\"))  # Charger les stop words pour le fran\u00e7ais\n",
        "nlp = spacy.load(\"fr_core_news_sm\")  # Chargement du mod\u00e8le de langue fran\u00e7aise de SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lecture des donn\u00e9es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>agree_count</th>\n",
              "      <th>disagree_count</th>\n",
              "      <th>neutral_count</th>\n",
              "      <th>agree_score</th>\n",
              "      <th>disagree_score</th>\n",
              "      <th>neutral_score</th>\n",
              "      <th>doNotCare</th>\n",
              "      <th>doNotUnderstand</th>\n",
              "      <th>doable</th>\n",
              "      <th>impossible</th>\n",
              "      <th>likeIt</th>\n",
              "      <th>noOpinion</th>\n",
              "      <th>noWay</th>\n",
              "      <th>platitudeAgree</th>\n",
              "      <th>platitudeDisagree</th>\n",
              "      <th>nb_interrogations</th>\n",
              "      <th>nb_exclamation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Il faut une surveillance des lieux touristique...</td>\n",
              "      <td>27</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Il faut donner des sacs poubelles et des cendr...</td>\n",
              "      <td>56</td>\n",
              "      <td>18</td>\n",
              "      <td>15</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Il faut r\u00e9compenser et mettre en avant les act...</td>\n",
              "      <td>35</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  agree_count  \\\n",
              "0  Il faut une surveillance des lieux touristique...           27   \n",
              "1  Il faut donner des sacs poubelles et des cendr...           56   \n",
              "2  Il faut r\u00e9compenser et mettre en avant les act...           35   \n",
              "\n",
              "   disagree_count  neutral_count  agree_score  disagree_score  neutral_score  \\\n",
              "0               7              7         0.65            0.18           0.17   \n",
              "1              18             15         0.62            0.19           0.19   \n",
              "2               7             14         0.62            0.13           0.25   \n",
              "\n",
              "   doNotCare  doNotUnderstand  doable  impossible  likeIt  noOpinion  noWay  \\\n",
              "0          1                0       8           2       2          0      3   \n",
              "1          0                0      21           6       6          5      4   \n",
              "2          0                5      12           2       2          2      3   \n",
              "\n",
              "   platitudeAgree  platitudeDisagree  nb_interrogations  nb_exclamation  \n",
              "0               3                  4                  0               0  \n",
              "1               5                  7                  0               0  \n",
              "2               1                  5                  0               0  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\n",
        "    \"../data/processed/req_tourisme_responsable.csv\", delimiter=\";\", encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "- Tokenisation avec NLTK (obtenir un decoupage pour chaque mot) pour garder les mots en majuscules\n",
        "- Tokenisation avec GENSIM en utilisant simple preprocess pour normaliser (enlever les accents, mettre en minuscules, ...)\n",
        "### Avec NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokeniser chaque document dans la colonne 'texte'\n",
        "df[\"tokens\"] = df[\"content\"].apply(lambda x: word_tokenize(x, language=\"french\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [Il, faut, une, surveillance, des, lieux, tour...\n",
              "1    [Il, faut, donner, des, sacs, poubelles, et, d...\n",
              "2    [Il, faut, r\u00e9compenser, et, mettre, en, avant,...\n",
              "3    [Il, faut, promouvoir, toutes, les, destinatio...\n",
              "4    [Il, faut, enrayer, la, sp\u00e9culation, immobili\u00e8...\n",
              "Name: tokens, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"tokens\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr\u00e9er la colonne 'case' en v\u00e9rifiant si chaque token est en majuscules ou en minuscules\n",
        "df[\"case\"] = df[\"tokens\"].apply(\n",
        "    lambda tokens: [\"Maj\" if token.isupper() else \"Min\" for token in tokens]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Avec GENSIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokeniser chaque document dans la colonne 'texte'\n",
        "df[\"tokens_normalized\"] = df[\"content\"].apply(\n",
        "    lambda x: simple_preprocess(x, deacc=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>case</th>\n",
              "      <th>tokens_normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
              "      <td>[il, faut, une, surveillance, des, lieux, tour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
              "      <td>[il, faut, donner, des, sacs, poubelles, et, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
              "      <td>[il, faut, recompenser, et, mettre, en, avant,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
              "      <td>[il, faut, promouvoir, toutes, les, destinatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Min, Min, Min, Min, Min, Min, Min, Min, Min, ...</td>\n",
              "      <td>[il, faut, enrayer, la, speculation, immobilie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                case  \\\n",
              "0  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
              "1  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
              "2  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
              "3  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
              "4  [Min, Min, Min, Min, Min, Min, Min, Min, Min, ...   \n",
              "\n",
              "                                   tokens_normalized  \n",
              "0  [il, faut, une, surveillance, des, lieux, tour...  \n",
              "1  [il, faut, donner, des, sacs, poubelles, et, d...  \n",
              "2  [il, faut, recompenser, et, mettre, en, avant,...  \n",
              "3  [il, faut, promouvoir, toutes, les, destinatio...  \n",
              "4  [il, faut, enrayer, la, speculation, immobilie...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[[\"case\", \"tokens_normalized\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Traitement des stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enlever les stop words des tokens normalis\u00e9s et stocker les mots retir\u00e9s dans une nouvelle colonne\n",
        "df[\"stop_words\"] = df[\"tokens_normalized\"].apply(\n",
        "    lambda tokens: [token for token in tokens if token in stop_words]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens_normalized</th>\n",
              "      <th>stop_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[il, faut, une, surveillance, des, lieux, tour...</td>\n",
              "      <td>[il, une, des, avec, la, pour, ou]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[il, faut, donner, des, sacs, poubelles, et, d...</td>\n",
              "      <td>[il, des, et, des, de, aux, des, et, en, de]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[il, faut, recompenser, et, mettre, en, avant,...</td>\n",
              "      <td>[il, et, en, les, des, du, et, des, pour]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[il, faut, promouvoir, toutes, les, destinatio...</td>\n",
              "      <td>[il, les, et, le]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[il, faut, enrayer, la, speculation, immobilie...</td>\n",
              "      <td>[il, la, en, de, pour, un]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   tokens_normalized  \\\n",
              "0  [il, faut, une, surveillance, des, lieux, tour...   \n",
              "1  [il, faut, donner, des, sacs, poubelles, et, d...   \n",
              "2  [il, faut, recompenser, et, mettre, en, avant,...   \n",
              "3  [il, faut, promouvoir, toutes, les, destinatio...   \n",
              "4  [il, faut, enrayer, la, speculation, immobilie...   \n",
              "\n",
              "                                     stop_words  \n",
              "0            [il, une, des, avec, la, pour, ou]  \n",
              "1  [il, des, et, des, de, aux, des, et, en, de]  \n",
              "2     [il, et, en, les, des, du, et, des, pour]  \n",
              "3                             [il, les, et, le]  \n",
              "4                    [il, la, en, de, pour, un]  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[[\"tokens_normalized\", \"stop_words\"]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enlever les deux premiers termes de chaque liste dans la colonne 'tokens_normalized'\n",
        "df[\"tokens_normalized\"] = df[\"tokens_normalized\"].apply(lambda tokens: tokens[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [une, surveillance, des, lieux, touristiques, ...\n",
              "1    [donner, des, sacs, poubelles, et, des, cendri...\n",
              "2    [recompenser, et, mettre, en, avant, les, acte...\n",
              "3    [promouvoir, toutes, les, destinations, et, de...\n",
              "4    [enrayer, la, speculation, immobiliere, en, te...\n",
              "Name: tokens_normalized, dtype: object"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"tokens_normalized\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enlever les stop words des tokens normalis\u00e9s\n",
        "df[\"tokens_final\"] = df[\"tokens_normalized\"].apply(\n",
        "    lambda tokens: [token for token in tokens if token not in stop_words]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [surveillance, lieux, touristiques, amende, cl...\n",
              "1    [donner, sacs, poubelles, cendriers, poche, va...\n",
              "2    [recompenser, mettre, avant, actes, projets, e...\n",
              "3    [promouvoir, toutes, destinations, developper,...\n",
              "4    [enrayer, speculation, immobiliere, territoire...\n",
              "Name: tokens_final, dtype: object"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"tokens_final\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lemmatisation avec spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction pour lemmatiser les tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    # Joindre les tokens en une cha\u00eene de caract\u00e8res\n",
        "    text = \" \".join(tokens)\n",
        "    # L'objet nlp traite le texte\n",
        "    doc = nlp(text)\n",
        "    # Lemmatiser chaque token\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "    return lemmatized_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Appliquer la lemmatisation \u00e0 chaque ligne de la colonne 'tokens_normalized'\n",
        "df[\"tokens_lemmatized\"] = df[\"tokens_final\"].apply(lambda x: lemmatize_tokens(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [surveillance, lieux, touristique, amende, cle...\n",
              "1    [donner, sac, poubelle, cendrier, poch, vacanc...\n",
              "2    [recompenser, mettre, avant, acte, projet, eco...\n",
              "3    [promouvoir, tout, destination, developper, to...\n",
              "4    [enrayer, speculation, immobilier, territoire,...\n",
              "Name: tokens_lemmatized, dtype: object"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"tokens_lemmatized\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Suite \u00e0 plusieurs visualisations de nuage de mots, enrichissement des stop_words\n",
        "- les d\u00e9rive de tourisme\n",
        "- tout\n",
        "- faire\n",
        "- plus\n",
        "\n",
        "Ces mots n'apportent pas grand-chose sur le sujet. Donc on les enl\u00e8ve des tokens lemmatis\u00e9s. Bien s\u00fbr, ce processus pourrait continuer de mani\u00e8re it\u00e9rative, mais il faut savoir s'arr\u00eater.\n",
        "\n",
        "\ud83d\udd0d Pourquoi j'ai choisi d'enlever les stop words avant vectorisation ?\n",
        "Les stop words (comme le, la, de, et, que...) :\n",
        "- n\u2019apportent pas d'information s\u00e9mantique forte ;\n",
        "- sont trop fr\u00e9quents \u2192 ils polluent l\u2019apprentissage de Word2Vec ;\n",
        "- g\u00e9n\u00e8rent des vecteurs tr\u00e8s proches entre eux, peu utiles pour des t\u00e2ches de similarit\u00e9 ou de clustering.\n",
        "\n",
        "Word2Vec apprend \u00e0 pr\u00e9dire un mot \u00e0 partir du contexte (ou l\u2019inverse). Les stop words, tr\u00e8s fr\u00e9quents, apparaissent dans presque tous les contextes \u2192 ils biaisent l\u2019apprentissage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words.add(\"tourisme\")\n",
        "stop_words.add(\"touristique\")\n",
        "stop_words.add(\"touriste\")\n",
        "stop_words.add(\"tourism\")\n",
        "stop_words.add(\"tout\")\n",
        "stop_words.add(\"plus\")\n",
        "stop_words.add(\"faire\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enlever les stop words des tokens normalis\u00e9s\n",
        "df[\"tokens_final\"] = df[\"tokens_lemmatized\"].apply(\n",
        "    lambda tokens: [token for token in tokens if token not in stop_words]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sauvegarde du dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Repertoire de sauvegarde\n",
        "savepath = \"../data/processed\"\n",
        "\n",
        "# Test de l'existence du r\u00e9pertoire\n",
        "if not os.path.exists(savepath):\n",
        "    # Cr\u00e9ation du r\u00e9pertoire s'il n'existe pas\n",
        "    os.makedirs(savepath)\n",
        "\n",
        "# Sauvegarde des jeux de donn\u00e9es\n",
        "df.to_csv(\n",
        "    path_or_buf=os.path.join(savepath, \"req_tourisme_responsable_tokenised.csv\"),\n",
        "    sep=\";\",\n",
        "    encoding=\"utf-8\",\n",
        "    index_label=False,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tourisme-vert-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
